Directory structure:
â””â”€â”€ gitreceiver-tama-mcp/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ prd.txt
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ pyproject.toml.bck
    â”œâ”€â”€ README_zh.md
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ .python-version
    â”œâ”€â”€ docs/
    â”‚   â””â”€â”€ cli_test_rules/
    â”‚       â”œâ”€â”€ tama_complete_test_plan.md
    â”‚       â”œâ”€â”€ tama_link_test_plan.md
    â”‚       â”œâ”€â”€ tama_list_test_plan.md
    â”‚       â”œâ”€â”€ tama_next_test_plan.md
    â”‚       â”œâ”€â”€ tama_remove-dep_test_plan.md
    â”‚       â”œâ”€â”€ tama_remove_test_plan.md
    â”‚       â””â”€â”€ tama_status_test_plan.md
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ exceptions.py
    â”‚   â”œâ”€â”€ git_utils.py
    â”‚   â”œâ”€â”€ mcp_server.py
    â”‚   â”œâ”€â”€ ai/
    â”‚   â”‚   â”œâ”€â”€ client.py
    â”‚   â”‚   â””â”€â”€ prompts.py
    â”‚   â”œâ”€â”€ cli/
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â””â”€â”€ ui.py
    â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ settings.py
    â”‚   â””â”€â”€ task_manager/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ complexity.py
    â”‚       â”œâ”€â”€ core.py
    â”‚       â”œâ”€â”€ data_models.py
    â”‚       â”œâ”€â”€ dependencies.py
    â”‚       â”œâ”€â”€ expansion.py
    â”‚       â”œâ”€â”€ file_generator.py
    â”‚       â”œâ”€â”€ parsing.py
    â”‚       â”œâ”€â”€ storage.py
    â”‚       â””â”€â”€ storage_sqlite.py
    â””â”€â”€ tests/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ fixtures/
        â”‚   â””â”€â”€ __init__.py
        â”œâ”€â”€ integration/
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ test_cli_commands.py
        â”‚   â””â”€â”€ test_cli_status.py
        â””â”€â”€ unit/
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ test_ai_client.py
            â”œâ”€â”€ test_complexity.py
            â”œâ”€â”€ test_core.py
            â”œâ”€â”€ test_dependencies.py
            â”œâ”€â”€ test_expansion.py
            â”œâ”€â”€ test_file_generator.py
            â””â”€â”€ test_storage.py

================================================
FILE: README.md
================================================
[Binary file]


================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 Gitreceiver

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: prd.txt
================================================
[Binary file]


================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "tama-cli"
version = "0.1.1"  # å»ºè®®æ›´æ–°ç‰ˆæœ¬å·ï¼Œé¿å…ä¸Žæ—§ç‰ˆæœ¬å†²çª
authors = [
  { name="Your Name", email="youremail@example.com" },
]
description = "ä¸€ä¸ªç”± AI é©±åŠ¨çš„ä»»åŠ¡ç®¡ç†å‘½ä»¤è¡Œå·¥å…·"
readme = "readme-0.2.1.md"
requires-python = ">=3.10"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "mcp[cli]>=1.6.0",
    "openai>=1.10.0",
    "pytest>=7.4.0",
    "pytest-mock>=3.10.0",
    "python-dotenv>=1.0.0",
    "rich>=13.0.0",
    "typer>=0.9.0",
    "pydantic",
]

[tool.pytest.ini_options]
pythonpath = ["src"]

[project.scripts]
tama = "cli.main:app"


================================================
FILE: pyproject.toml.bck
================================================
[project]
name = "tama"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "mcp[cli]>=1.6.0",
    "openai>=1.10.0",
    "pytest>=7.4.0",
    "pytest-mock>=3.10.0",
    "python-dotenv>=1.0.0",
    "rich>=13.0.0",
    "typer>=0.9.0",
    "pydantic",
]

[tool.pytest.ini_options]
pythonpath = ["src"]

[project.scripts]
tama = "cli.main:app"



================================================
FILE: README_zh.md
================================================
[Binary file]


================================================
FILE: requirements.txt
================================================
openai>=1.10.0
python-dotenv>=1.0.0
typer>=0.9.0
rich>=13.0.0
pytest>=7.4.0
pytest-mock>=3.10.0
mcp[cli]>=1.6.0



================================================
FILE: .env.example
================================================
# DeepSeek Configuration
DEEPSEEK_API_KEY="your-deepseek-api"
DEEPSEEK_BASE_URL="https://api.deepseek.com" 
# Optional: Default is set in settings

# Model Names (confirm exact identifiers from DeepSeek docs)
DEEPSEEK_GENERAL_MODEL="deepseek-chat"
DEEPSEEK_REASONING_MODEL="deepseek-reasoner"

# Optional AI Settings
# AI_MAX_TOKENS=8192
# AI_TEMPERATURE=0.7

# Application Settings
LOG_LEVEL="INFO" # DEBUG, INFO, WARNING, ERROR
DEFAULT_PRIORITY="medium"
DEFAULT_SUBTASKS=3
PROJECT_NAME="My Task Manager Project"
DEBUG=False 

PROPAGATE_STATUS_CHANGE = False # optinal,default is False


================================================
FILE: .python-version
================================================
3.12



================================================
FILE: docs/cli_test_rules/tama_complete_test_plan.md
================================================
# Test Plan for `tama complete`

This document outlines the test plan for the `tama complete` command.

## Command Synopsis

`tama complete <task_id> [--commit] [--propagate]`

## Test Summary (Executed on 2025-11-08)

During testing, two bugs were identified:
1.  The command would fail with a `ModuleNotFoundError` because the `git_utils.py` file was missing.
2.  The command would output a redundant success message even if the task was already completed.

Both bugs have been **fixed**. The `git_utils.py` file has been restored, and the command now correctly handles already completed tasks. All test cases now pass as expected.

## Test Cases

### 1. Happy Path

- **Test Case 1.1**: Complete a main task.
  - **Prerequisites**: A task exists with ID 1, in "in-progress" status.
  - **Command**: `uv run tama complete 1`
  - **Expected Outcome**: The command succeeds. A message confirms the status is now "done". `tama show 1` verifies the new status.

- **Test Case 1.2**: Complete a task with `--commit`.
  - **Prerequisites**: A task exists with ID 1, in "in-progress" status. There are staged changes in Git.
  - **Command**: `uv run tama complete 1 --commit`
  - **Expected Outcome**: The command succeeds. The task status is updated to "done". A new Git commit is created with a message like "feat: Complete task 1 - <task-title>".

### 2. Error Cases

- **Test Case 2.1**: Task not found.
  - **Prerequisites**: No task with the specified ID exists.
  - **Command**: `uv run tama complete 999`
  - **Expected Outcome**: The command fails with a clear error message stating that the task with ID 999 was not found.

- **Test Case 2.2**: Task already completed.
  - **Prerequisites**: A task exists with ID 1, already in "done" status.
  - **Command**: `uv run tama complete 1`
  - **Expected Outcome**: The command exits gracefully with a message indicating the task is already done. No status change occurs.

### 3. Boundary and Edge Cases

- **Test Case 3.1**: Status propagation to subtasks.
  - **Prerequisites**: A parent task with ID 1 has several subtasks (`1.1`, `1.2`), all in "in-progress" status.
  - **Command**: `uv run tama complete 1 --propagate`
  - **Expected Outcome**: The command succeeds. `tama show 1` confirms that the parent task AND all of its subtasks are now "done".

- **Test Case 3.2**: Status propagation is disabled by default.
  - **Prerequisites**: Same as 3.1.
  - **Command**: `uv run tama complete 1` (without `--propagate`)
  - **Expected Outcome**: The command succeeds. Only the parent task's status is changed. The subtasks remain "in-progress".



================================================
FILE: docs/cli_test_rules/tama_link_test_plan.md
================================================
# Test Plan for `tama link`

## 1. Basic Functionality

*   **Test Case 1.1: Link two existing tasks**
    *   **Description**: Test linking two existing tasks.
    *   **Setup**:
        1.  Add task "Task A".
        2.  Add task "Task B".
    *   **Command**: `tama link <task_A_id> <task_B_id>`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   Task B should now have a dependency on Task A.
        *   Running `tama next` should propose Task A, not Task B.
        *   `tama detail <task_B_id>` should show Task A as a dependency.

*   **Test Case 1.2: Link a task to a completed task**
    *   **Description**: Test linking a pending task to a completed task.
    *   **Setup**:
        1.  Add task "Task A" and mark it as complete.
        2.  Add task "Task B".
    *   **Command**: `tama link <task_B_id> <task_A_id>`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   Task B should still be available as the next task, as its dependency is already complete.
        *   `tama detail <task_B_id>` should show Task A as a dependency.

## 2. Edge Cases & Error Handling

*   **Test Case 2.1: Link a task to itself**
    *   **Description**: Test linking a task to itself.
    *   **Setup**:
        1.  Add task "Task A".
    *   **Command**: `tama link <task_A_id> <task_A_id>`
    *   **Expected Outcome**:
        *   The command should fail with an error message indicating a task cannot depend on itself.

*   **Test Case 2.2: Link a non-existent task**
    *   **Description**: Test linking to or from a task ID that does not exist.
    *   **Setup**:
        1.  Add task "Task A".
    *   **Command 1**: `tama link <task_A_id> 999`
    *   **Command 2**: `tama link 999 <task_A_id>`
    *   **Expected Outcome**:
        *   Both commands should fail with an error message indicating the task ID was not found.

*   **Test Case 2.3: Circular Dependencies**
    *   **Description**: Test creating a circular dependency.
    *   **Setup**:
        1.  Add task "Task A".
        2.  Add task "Task B".
        3.  Link Task B to Task A (`tama link <task_B_id> <task_A_id>`).
    *   **Command**: `tama link <task_A_id> <task_B_id>`
    *   **Expected Outcome**:
        *   The command should fail with an error message warning about creating a circular dependency.

*   **Test Case 2.4: Incorrect number of arguments**
    *   **Description**: Test calling the command with too few or too many arguments.
    *   **Command 1**: `tama link`
    *   **Command 2**: `tama link 1`
    *   **Command 3**: `tama link 1 2 3`
    *   **Expected Outcome**:
        *   All commands should fail and show the correct usage/help text.



================================================
FILE: docs/cli_test_rules/tama_list_test_plan.md
================================================
# Test Plan for `tama list`

## 1. Basic Functionality

*   **Test Case 1.1: List all tasks**
    *   **Description**: Test listing all tasks when no filters are applied.
    *   **Setup**:
        1.  Add task "Task A" (status: pending, priority: medium).
        2.  Add task "Task B" (status: in-progress, priority: high).
        3.  Add task "Task C" (status: done, priority: low).
    *   **Command**: `tama list`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   The output should be a table displaying all three tasks with their correct details.

## 2. Filtering

*   **Test Case 2.1: Filter by status**
    *   **Description**: Test filtering the task list by status.
    *   **Setup**:
        1.  Add task "Task A" (status: pending).
        2.  Add task "Task B" (status: in-progress).
        3.  Add task "Task C" (status: pending).
    *   **Command**: `tama list --status pending`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   The output should only display "Task A" and "Task C".

*   **Test Case 2.2: Filter by priority**
    *   **Description**: Test filtering the task list by priority.
    *   **Setup**:
        1.  Add task "Task A" (priority: high).
        2.  Add task "Task B" (priority: medium).
        3.  Add task "Task C" (priority: high).
    *   **Command**: `tama list --priority high`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   The output should only display "Task A" and "Task C".

*   **Test Case 2.3: Filter by both status and priority**
    *   **Description**: Test filtering the task list by both status and priority simultaneously.
    *   **Setup**:
        1.  Add task "Task A" (status: pending, priority: high).
        2.  Add task "Task B" (status: pending, priority: medium).
        3.  Add task "Task C" (status: in-progress, priority: high).
        4.  Add task "Task D" (status: pending, priority: high).
    *   **Command**: `tama list --status pending --priority high`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   The output should only display "Task A" and "Task D".

## 3. Edge Cases

*   **Test Case 3.1: List with no tasks**
    *   **Description**: Test the output when no tasks have been added.
    *   **Setup**:
        1.  Ensure the task list is empty.
    *   **Command**: `tama list`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   The output should indicate that no tasks were found or display an empty table.

*   **Test Case 3.2: Filter with no matching tasks**
    *   **Description**: Test the output when a filter is applied that matches no tasks.
    *   **Setup**:
        1.  Add task "Task A" (status: pending, priority: medium).
    *   **Command**: `tama list --status done`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   The output should indicate that no tasks were found or display an empty table.



================================================
FILE: docs/cli_test_rules/tama_next_test_plan.md
================================================
# Test Plan for `tama next`

This document outlines the test plan for the `tama next` command.

## Command Synopsis

`tama next`

## Test Cases

### 1. Happy Path

- **Test Case 1.1**: Find the next available task.
  - **Prerequisites**: Several tasks exist. Task 1 is "done". Task 2 is "pending" with high priority and no dependencies. Task 3 is "pending" with medium priority.
  - **Command**: `uv run tama next`
  - **Expected Outcome**: The command succeeds and displays the details for Task 2, as it is the highest-priority pending task.

- **Test Case 1.2**: Dependency blocking.
  - **Prerequisites**: Task 1 is "pending". Task 2 is "pending" and depends on Task 1.
  - **Command**: `uv run tama next`
  - **Expected Outcome**: The command displays the details for Task 1. Task 2 is not shown because its dependency is not met.

### 2. Boundary and Edge Cases

- **Test Case 2.1**: No eligible tasks.
  - **Prerequisites**: All tasks are either "done" or "in-progress", or are blocked by pending dependencies.
  - **Command**: `uv run tama next`
  - **Expected Outcome**: The command exits gracefully with a message indicating that no eligible tasks were found.

- **Test Case 2.2**: No tasks at all.
  - **Prerequisites**: The task database is empty.
  - **Command**: `uv run tama next`
  - **Expected Outcome**: The command exits gracefully with a message indicating that no tasks were found.

- **Test Case 2.3**: Dependency as a string.
  - **Prerequisites**: Task 1 is "pending". Task 2 is "pending" and depends on Task 1, with the dependency specified as a string ("1").
  - **Command**: `uv run tama next`
  - **Expected Outcome**: The command displays the details for Task 1. Task 2 is not shown because its dependency is not met. This test case was added to address a bug where string dependencies were not correctly parsed.



================================================
FILE: docs/cli_test_rules/tama_remove-dep_test_plan.md
================================================
# Test Plan for `tama remove-dep`

## 1. Basic Functionality

*   **Test Case 1.1: Remove an existing dependency**
    *   **Description**: Test removing a dependency from a task.
    *   **Setup**:
        1.  Add task "Task A".
        2.  Add task "Task B".
        3.  Make Task B dependent on Task A (`tama add-dep <task_B_id> <task_A_id>`).
    *   **Command**: `tama remove-dep <task_B_id> <task_A_id>`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   `tama show <task_B_id>` should no longer list Task A as a dependency.
        *   Running `tama next` should now consider both Task A and Task B as eligible (depending on priority/ID).

## 2. Edge Cases & Error Handling

*   **Test Case 2.1: Remove a dependency that does not exist**
    *   **Description**: Test trying to remove a dependency that isn't set.
    *   **Setup**:
        1.  Add task "Task A".
        2.  Add task "Task B".
    *   **Command**: `tama remove-dep <task_B_id> <task_A_id>`
    *   **Expected Outcome**:
        *   The command should fail or give a warning, indicating that the dependency does not exist.

*   **Test Case 2.2: Remove a dependency from a non-existent task**
    *   **Description**: Test calling the command with a target task ID that does not exist.
    *   **Setup**:
        1.  Add task "Task A".
    *   **Command**: `tama remove-dep 999 <task_A_id>`
    *   **Expected Outcome**:
        *   The command should fail with an error message indicating the target task was not found.

*   **Test Case 2.3: Remove a non-existent dependency ID from an existing task**
    *   **Description**: Test calling the command with a dependency task ID that does not exist.
    *   **Setup**:
        1.  Add task "Task A".
    *   **Command**: `tama remove-dep <task_A_id> 999`
    *   **Expected Outcome**:
        *   The command should fail or give a warning, indicating that the dependency ID to be removed was not found.

*   **Test Case 2.4: Incorrect number of arguments**
    *   **Description**: Test calling the command with too few arguments.
    *   **Command 1**: `tama remove-dep`
    *   **Command 2**: `tama remove-dep 1`
    *   **Expected Outcome**:
        *   Both commands should fail and show the correct usage/help text.



================================================
FILE: docs/cli_test_rules/tama_remove_test_plan.md
================================================
# Test Plan for `tama remove`

## 1. Basic Functionality

*   **Test Case 1.1: Remove a single, simple task**
    *   **Description**: Test removing a standalone task with no dependencies or subtasks.
    *   **Setup**:
        1.  Add task "Task A".
        2.  Add task "Task B".
    *   **Command**: `tama remove <task_A_id>`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   `tama list` should no longer show Task A, only Task B.

*   **Test Case 1.2: Remove a task with subtasks**
    *   **Description**: Test removing a parent task that contains subtasks.
    *   **Setup**:
        1.  Add task "Task A".
        2.  Add subtask "Subtask A.1" to Task A.
    *   **Command**: `tama remove <task_A_id>`
    *   **Expected Outcome**:
        *   The command should execute directly, without a confirmation prompt.
        *   Both the parent task and all its subtasks should be removed.
        *   `tama list` should show no tasks.

*   **Test Case 1.3: Remove just a subtask**
    *   **Description**: Test removing a specific subtask without affecting the parent task.
    *   **Setup**:
        1.  Add task "Task A".
        2.  Add subtask "Subtask A.1" to Task A.
    *   **Command**: `tama remove <subtask_A.1_id>` (e.g., `1.1`)
    *   **Expected Outcome**:
        *   Only the subtask is removed.
        *   The parent task "Task A" remains.
        *   `tama show <task_A_id>` should no longer list the removed subtask.

## 2. Edge Cases & Error Handling

*   **Test Case 2.1: Remove a non-existent task**
    *   **Description**: Test attempting to remove a task ID that does not exist.
    *   **Setup**:
        1.  Ensure no task with ID 999 exists.
    *   **Command**: `tama remove 999`
    *   **Expected Outcome**:
        *   The command should fail with an error message indicating the task was not found.

*   **Test Case 2.2: Remove a task that is a dependency for another task**
    *   **Description**: Test removing a task that another task depends on.
    *   **Setup**:
        1.  Add task "Task A".
        2.  Add task "Task B".
        3.  Make Task B dependent on Task A (`tama add-dep <task_B_id> <task_A_id>`).
    *   **Command**: `tama remove <task_A_id>`
    *   **Expected Outcome**:
        *   The command should execute directly, without a confirmation prompt.
        *   A message should indicate that the dependent task was updated.
        *   The task should be removed.
        *   The dependency link should be removed from Task B. `tama show <task_B_id>` should no longer show a dependency on Task A.

*   **Test Case 2.3: Incorrect number of arguments**
    *   **Description**: Test calling the command with too few arguments.
    *   **Command**: `tama remove`
    *   **Expected Outcome**:
        *   The command should fail and show the correct usage/help text.



================================================
FILE: docs/cli_test_rules/tama_status_test_plan.md
================================================
# Test Plan for `tama status`

## 1. Basic Functionality

*   **Test Case 1.1: Set a task's status**
    *   **Description**: Test changing the status of a task from 'pending' to 'in-progress'.
    *   **Setup**:
        1.  Add task "Task A".
    *   **Command**: `tama status <task_A_id> in-progress`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   `tama show <task_A_id>` should display the status as 'in-progress'.

*   **Test Case 1.2: Set a subtask's status**
    *   **Description**: Test changing the status of a subtask.
    *   **Setup**:
        1.  Add task "Task A".
        2.  Add subtask "Subtask A.1" to Task A.
    *   **Command**: `tama status <subtask_A.1_id> done`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   `tama show <task_A_id>` should show the subtask with the status 'done'.

## 2. Edge Cases & Error Handling

*   **Test Case 2.1: Set status on a non-existent task**
    *   **Description**: Test attempting to set the status for a task ID that does not exist.
    *   **Setup**:
        1.  Ensure no task with ID 999 exists.
    *   **Command**: `tama status 999 pending`
    *   **Expected Outcome**:
        *   The command should fail with an error message indicating the task was not found.

*   **Test Case 2.2: Use an invalid status value**
    *   **Description**: Test the command with a status that is not one of the allowed values (pending, in-progress, done, blocked).
    *   **Setup**:
        1.  Add task "Task A".
    *   **Command**: `tama status <task_A_id> invalid-status`
    *   **Expected Outcome**:
        *   The command should fail with an error message indicating that 'invalid-status' is not a valid choice.

*   **Test Case 2.3: Incorrect number of arguments**
    *   **Description**: Test calling the command with too few or too many arguments.
    *   **Command 1**: `tama status`
    *   **Command 2**: `tama status 1`
    *   **Command 3**: `tama status 1 pending extra-arg`
    *   **Expected Outcome**:
        *   All commands should fail and show the correct usage/help text.

## 3. Regression Tests

*   **Test Case 3.1: Parent task status updates when last subtask is completed**
    *   **Description**: Test that when the only subtask of a parent task is marked as 'done', the parent task's status is also updated to 'done'. This is a regression test for a bug that caused an `AttributeError`.
    *   **Setup**:
        1.  Add task "Task A".
        2.  Add subtask "Subtask A.1" to Task A.
    *   **Command**: `tama status <subtask_A.1_id> done`
    *   **Expected Outcome**:
        *   The command should execute successfully.
        *   The parent task's status should automatically be updated to 'done'.



================================================
FILE: src/__init__.py
================================================
[Empty file]


================================================
FILE: src/exceptions.py
================================================
class TaskManagerError(Exception):
    """Base exception class for Task Manager errors."""
    pass

class AIResponseParsingError(TaskManagerError):
    """Raised when the AI response cannot be parsed correctly."""
    pass

class ParentTaskNotFoundError(TaskManagerError):
    """Raised when a specified parent task cannot be found."""
    pass

class TaskNotFoundError(TaskManagerError):
    """Raised when a specified task cannot be found."""
    pass

class InvalidStatusError(TaskManagerError):
    """Raised when an invalid status is provided."""
    pass

class DependencyError(TaskManagerError):
    """Raised for errors related to task dependencies."""
    pass

class ConfigurationError(TaskManagerError):
    """Raised for errors related to configuration loading or validation."""
    pass

class FileOperationError(TaskManagerError):
    """Raised for errors during file operations."""
    pass

class InputValidationError(TaskManagerError):
    """Raised for errors during input validation."""
    pass



================================================
FILE: src/git_utils.py
================================================
import subprocess
from rich.console import Console

console = Console()

def create_branch(branch_name: str):
    """Creates and switches to a new Git branch."""
    try:
        subprocess.run(["git", "checkout", "-b", branch_name], check=True, capture_output=True, text=True)
        console.print(f"[bold green]Switched to a new branch '{branch_name}'[/bold green]")
    except subprocess.CalledProcessError as e:
        console.print(f"[bold red]Error creating branch '{branch_name}': {e.stderr}[/bold red]")

def commit_changes(message: str):
    """Commits staged changes with a given message."""
    try:
        result = subprocess.run(["git", "commit", "-m", message], check=True, capture_output=True, text=True)
        console.print("[bold green]Changes committed.[/bold green]")
        console.print(result.stdout)
    except subprocess.CalledProcessError as e:
        console.print(f"[bold red]Error committing changes: {e.stderr}[/bold red]")



================================================
FILE: src/mcp_server.py
================================================
#!/usr/bin/env python
import logging
import os
import base64
import subprocess
from typing import List, Optional, Union, Tuple

from mcp.server.fastmcp import FastMCP, Context

# Absolute imports
from task_manager.core import (
    get_task_by_id as core_get_task_by_id,
    find_next_task as core_find_next_task,
    set_task_status as core_set_task_status,
    add_new_task as core_add_new_task,
    add_subtask as core_add_subtask,
    remove_item as core_remove_item,
    add_dependency as core_add_dependency,
    remove_single_dependency as core_remove_single_dependency,
)
from task_manager.data_models import Task, Subtask, Status, Priority, Dependency, TasksData
from task_manager.storage import load_tasks, save_tasks
from task_manager.dependencies import find_circular_dependencies
from config import settings

logging.basicConfig(level=logging.INFO if not settings.DEBUG else logging.DEBUG)
logger = logging.getLogger(__name__)

# --- State Management (Simple Global Variable Approach) ---

# Determine the task file path
TASK_FILE = settings.TASKS_JSON_PATH
logger.info(f"Using task file: {TASK_FILE}")

# Load initial tasks
if os.path.exists(TASK_FILE):
    tasks_data = load_tasks()
    tasks_list: List[Task] = tasks_data.tasks
    logger.info(f"Loaded {len(tasks_list)} tasks from {TASK_FILE}")
else:
    tasks_list: List[Task] = []
    logger.warning(f"Task file {TASK_FILE} not found. Starting with empty list.")

# Helper to save tasks after modification
def _save_current_tasks():
    try:
        tasks_data = TasksData(tasks=tasks_list)
        save_tasks(tasks_data)
        logger.debug(f"Saved {len(tasks_list)} tasks to {TASK_FILE}")
        return True
    except Exception as e:
        logger.error(f"Failed to save tasks to {TASK_FILE}: {e}", exc_info=settings.DEBUG)
        return False

# --- MCP Server Definition ---
mcp = FastMCP("TAMA Task Manager", description="MCP server for managing TAMA tasks.")

# --- Task Management Tools ---

@mcp.tool()
def list_tasks(
    status: Optional[str] = None,
    priority: Optional[str] = None
) -> str:
    """
    åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡ï¼Œå¯é€‰æŒ‰çŠ¶æ€æˆ–ä¼˜å…ˆçº§è¿‡æ»¤ï¼Œè¿”å›žmarkdownæ ¼å¼çš„åˆ—è¡¨ã€‚
    
    Args:
        status: å¯é€‰çš„çŠ¶æ€è¿‡æ»¤ (pending, in-progress, done, deferred, blocked, review)
        priority: å¯é€‰çš„ä¼˜å…ˆçº§è¿‡æ»¤ (high, medium, low)
        
    Returns:
        markdownæ ¼å¼çš„ä»»åŠ¡åˆ—è¡¨å­—ç¬¦ä¸²
    """
    filtered_tasks = tasks_list.copy()
    
    if status:
        filtered_tasks = [t for t in filtered_tasks if t.status == status]
    if priority:
        filtered_tasks = [t for t in filtered_tasks if t.priority == priority]
    
    # çŠ¶æ€å¯¹åº”çš„emoji
    status_emoji = {
        "done": "âœ…",
        "pending": "âšª",
        "in-progress": "â³",
        "blocked": "â›”",
        "deferred": "ðŸ“…",
        "review": "ðŸ”"
    }
    
    # ä¼˜å…ˆçº§å¯¹åº”çš„emoji
    priority_emoji = {
        "high": "ðŸ”´",
        "medium": "ðŸŸ¡",
        "low": "ðŸŸ¢"
    }
    
    # ç”Ÿæˆmarkdownæ ¼å¼çš„åˆ—è¡¨
    markdown = "# ä»»åŠ¡åˆ—è¡¨\n\n"
    
    # æ·»åŠ è¿‡æ»¤ä¿¡æ¯
    filters = []
    if status:
        filters.append(f"çŠ¶æ€: {status_emoji.get(status, '')} {status}")
    if priority:
        filters.append(f"ä¼˜å…ˆçº§: {priority_emoji.get(priority, '')} {priority}")
    if filters:
        markdown += "**è¿‡æ»¤æ¡ä»¶:** " + ", ".join(filters) + "\n\n"
    
    if not filtered_tasks:
        markdown += "*æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä»»åŠ¡*\n"
        return markdown
    
    # æ·»åŠ è¡¨å¤´
    markdown += "| ID | æ ‡é¢˜ | çŠ¶æ€ | ä¼˜å…ˆçº§ | ä¾èµ–é¡¹ | å­ä»»åŠ¡æ•° |\n"
    markdown += "|:---|:-----|:-----|:-------|:-------|:---------|\n"
    
    # æ·»åŠ ä»»åŠ¡è¡Œ
    for task in filtered_tasks:
        task_id = str(task.id)
        title = task.title
        status = f"{status_emoji.get(task.status, '')} {task.status}"
        priority = f"{priority_emoji.get(task.priority, '')} {task.priority}"
        dependencies = ", ".join(map(str, task.dependencies)) if task.dependencies else "-"
        subtasks_count = len(task.subtasks)
        
        markdown += f"| {task_id} | {title} | {status} | {priority} | {dependencies} | {subtasks_count} |\n"
        
        # æ·»åŠ å­ä»»åŠ¡ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
        for subtask in task.subtasks:
            subtask_id = f"{task.id}.{subtask.id}"
            subtask_title = f"â””â”€ {subtask.title}"
            subtask_status = f"{status_emoji.get(subtask.status, '')} {subtask.status}"
            subtask_priority = f"{priority_emoji.get(subtask.priority, '')} {subtask.priority}"
            subtask_deps = ", ".join(map(str, subtask.dependencies)) if subtask.dependencies else "-"
            
            markdown += f"| {subtask_id} | {subtask_title} | {subtask_status} | {subtask_priority} | {subtask_deps} | - |\n"
    
    return markdown

@mcp.tool()
def show_task(task_id: str) -> Optional[Union[Task, Subtask]]:
    """
    æ˜¾ç¤ºæŒ‡å®šä»»åŠ¡æˆ–å­ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ã€‚
    
    Args:
        task_id: ä»»åŠ¡æˆ–å­ä»»åŠ¡çš„ID (ä¾‹å¦‚: "1" æˆ– "1.2")
        
    Returns:
        ä»»åŠ¡æˆ–å­ä»»åŠ¡å¯¹è±¡ï¼Œå¦‚æžœæœªæ‰¾åˆ°åˆ™è¿”å›žNone
    """
    return core_get_task_by_id(tasks_list, task_id)

@mcp.tool()
def set_status(task_id: str, new_status: str) -> bool:
    """
    è®¾ç½®ä»»åŠ¡æˆ–å­ä»»åŠ¡çš„çŠ¶æ€ã€‚
    
    Args:
        task_id: ä»»åŠ¡æˆ–å­ä»»åŠ¡çš„ID (ä¾‹å¦‚: "1" æˆ– "1.2")
        new_status: æ–°çŠ¶æ€ (pending, in-progress, done, deferred, blocked, review)
        
    Returns:
        æ›´æ–°æˆåŠŸè¿”å›žTrueï¼Œå¤±è´¥è¿”å›žFalse
    """
    if core_set_task_status(tasks_list, task_id, new_status):
        return _save_current_tasks()
    return False

@mcp.tool()
def next_task() -> Optional[Task]:
    """
    æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå¯æ‰§è¡Œçš„ä»»åŠ¡ã€‚
    
    Returns:
        ä¸‹ä¸€ä¸ªå¯æ‰§è¡Œçš„ä»»åŠ¡ï¼Œå¦‚æžœæ²¡æœ‰åˆ™è¿”å›žNone
    """
    return core_find_next_task(tasks_list)

@mcp.tool()
def add_task(
    title: str,
    description: Optional[str] = None,
    priority: Priority = settings.DEFAULT_PRIORITY,
    dependencies: List[Dependency] = []
) -> Optional[Task]:
    """
    æ·»åŠ æ–°çš„ä¸»ä»»åŠ¡ã€‚
    
    Args:
        title: ä»»åŠ¡æ ‡é¢˜
        description: å¯é€‰çš„ä»»åŠ¡æè¿°
        priority: ä»»åŠ¡ä¼˜å…ˆçº§ (high, medium, low)
        dependencies: ä¾èµ–ä»»åŠ¡IDåˆ—è¡¨
        
    Returns:
        æ–°åˆ›å»ºçš„ä»»åŠ¡å¯¹è±¡ï¼Œå¤±è´¥è¿”å›žNone
    """
    try:
        new_task = core_add_new_task(tasks_list, title, description, priority, dependencies)
        if _save_current_tasks():
            return new_task
        return None
    except Exception as e:
        logger.error(f"Error in add_task tool: {e}", exc_info=settings.DEBUG)
        return None

@mcp.tool()
def add_subtask(
    parent_id: str,
    title: str,
    description: Optional[str] = None,
    priority: Priority = settings.DEFAULT_PRIORITY,
    dependencies: List[Dependency] = []
) -> Optional[Subtask]:
    """
    æ·»åŠ å­ä»»åŠ¡ã€‚
    
    Args:
        parent_id: çˆ¶ä»»åŠ¡ID
        title: å­ä»»åŠ¡æ ‡é¢˜
        description: å¯é€‰çš„å­ä»»åŠ¡æè¿°
        priority: å­ä»»åŠ¡ä¼˜å…ˆçº§ (high, medium, low)
        dependencies: ä¾èµ–ä»»åŠ¡IDåˆ—è¡¨
        
    Returns:
        æ–°åˆ›å»ºçš„å­ä»»åŠ¡å¯¹è±¡ï¼Œå¤±è´¥è¿”å›žNone
    """
    try:
        parent_id_int = int(parent_id)
        new_subtask = core_add_subtask(tasks_list, parent_id_int, title, description, priority, dependencies)
        if _save_current_tasks():
            return new_subtask
        return None
    except Exception as e:
        logger.error(f"Error in add_subtask tool: {e}", exc_info=settings.DEBUG)
        return None

@mcp.tool()
def remove_item(task_id: str) -> Tuple[bool, List[Tuple[str, str]]]:
    """
    åˆ é™¤ä»»åŠ¡æˆ–å­ä»»åŠ¡ã€‚
    
    Args:
        task_id: è¦åˆ é™¤çš„ä»»åŠ¡æˆ–å­ä»»åŠ¡ID (ä¾‹å¦‚: "1" æˆ– "1.2")
        
    Returns:
        (æ˜¯å¦åˆ é™¤æˆåŠŸ, å—å½±å“çš„ä¾èµ–é¡¹åˆ—è¡¨)
    """
    success, affected_deps = core_remove_item(tasks_list, task_id)
    if success:
        if _save_current_tasks():
            return True, affected_deps
    return False, []

@mcp.tool()
def add_dependency(task_id: str, dependency_id: str) -> bool:
    """
    ä¸ºä»»åŠ¡æ·»åŠ ä¾èµ–é¡¹ã€‚
    
    Args:
        task_id: è¦æ·»åŠ ä¾èµ–çš„ä»»åŠ¡ID (ä¾‹å¦‚: "1" æˆ– "1.2")
        dependency_id: è¦æ·»åŠ çš„ä¾èµ–ä»»åŠ¡ID (ä¾‹å¦‚: "1" æˆ– "1.2")
        
    Returns:
        æ·»åŠ æˆåŠŸè¿”å›žTrueï¼Œå¤±è´¥è¿”å›žFalse
    """
    if core_add_dependency(tasks_list, task_id, dependency_id):
        return _save_current_tasks()
    return False

@mcp.tool()
def remove_dependency(task_id: str, dependency_id: str) -> bool:
    """
    ä»Žä»»åŠ¡ä¸­ç§»é™¤ä¾èµ–é¡¹ã€‚
    
    Args:
        task_id: è¦ç§»é™¤ä¾èµ–çš„ä»»åŠ¡ID (ä¾‹å¦‚: "1" æˆ– "1.2")
        dependency_id: è¦ç§»é™¤çš„ä¾èµ–ä»»åŠ¡ID (ä¾‹å¦‚: "1" æˆ– "1.2")
        
    Returns:
        ç§»é™¤æˆåŠŸè¿”å›žTrueï¼Œå¤±è´¥è¿”å›žFalse
    """
    if core_remove_single_dependency(tasks_list, task_id, dependency_id):
        return _save_current_tasks()
    return False

@mcp.tool()
def check_dependencies() -> Optional[List[str]]:
    """
    æ£€æŸ¥ä»»åŠ¡ä¸­æ˜¯å¦å­˜åœ¨å¾ªçŽ¯ä¾èµ–ã€‚
    
    Returns:
        å¦‚æžœå­˜åœ¨å¾ªçŽ¯ä¾èµ–ï¼Œè¿”å›žå¾ªçŽ¯è·¯å¾„ï¼›å¦åˆ™è¿”å›žNone
    """
    return find_circular_dependencies(tasks_list)

# --- Run the Server ---
if __name__ == "__main__":
    logger.info("Starting TAMA MCP server...")
    # Add necessary dependencies if they aren't automatically picked up (optional)
    # mcp.dependencies = ["pydantic", "rich", ...] # Add if needed
    mcp.run()
    logger.info("TAMA MCP server stopped.")



================================================
FILE: src/ai/client.py
================================================
import logging
import time
from typing import List, Dict, Any, Optional
from openai import OpenAI, APITimeoutError, APIConnectionError, RateLimitError

# Absolute imports
from config import settings
import ai.prompts as prompts

logger = logging.getLogger(__name__)

# Initialize OpenAI client for DeepSeek
if not settings.DEEPSEEK_API_KEY:
    logger.error("DEEPSEEK_API_KEY not found in settings. Please check your .env file.")
    client = None
else:
    try:
        client = OpenAI(
            api_key=settings.DEEPSEEK_API_KEY,
            base_url=settings.DEEPSEEK_BASE_URL
        )
        logger.info(f"OpenAI client initialized for DeepSeek at {settings.DEEPSEEK_BASE_URL}")
    except Exception as e:
        logger.exception("Failed to initialize OpenAI client for DeepSeek.")
        client = None

# --- Core API Call Function ---

def call_deepseek(
    model: str,
    messages: List[Dict[str, str]],
    max_retries: int = 3,
    retry_delay: int = 5,
    **kwargs # Pass other parameters like temperature, max_tokens
) -> Optional[str]:
    """
    Calls the DeepSeek API using the OpenAI client with retry logic.

    Args:
        model: The model name to use (e.g., settings.DEEPSEEK_GENERAL_MODEL).
        messages: A list of message dictionaries (e.g., [{"role": "user", "content": "..."}]).
        max_retries: Maximum number of retries on specific errors.
        retry_delay: Delay in seconds between retries.
        **kwargs: Additional arguments for chat completions create (temperature, max_tokens).

    Returns:
        The content of the response message or None if an error occurs after retries.
    """
    
    # Original code starts here
    if not client:
        logger.error("DeepSeek client not initialized. Cannot make API call.")
        return None
    
    attempt = 0
    while attempt < max_retries:
        try:
            logger.debug(f"Calling DeepSeek model '{model}' (Attempt {attempt + 1}/{max_retries}). Messages: {messages}")
            api_params = {
                "temperature": settings.AI_TEMPERATURE,
                "max_tokens": settings.AI_MAX_TOKENS,
                **kwargs 
            }
            completion = client.chat.completions.create(
                model=model,
                messages=messages,
                **api_params
            )
            response_content = completion.choices[0].message.content
            logger.debug(f"DeepSeek API call successful. Response: {response_content[:100]}...")
            return response_content.strip() if response_content else None

        except (APITimeoutError, APIConnectionError, RateLimitError) as e:
            attempt += 1
            logger.warning(f"DeepSeek API error ({type(e).__name__}), attempt {attempt}/{max_retries}. Retrying in {retry_delay}s...")
            if attempt >= max_retries:
                logger.error(f"DeepSeek API call failed after {max_retries} retries: {e}")
                return None
            time.sleep(retry_delay)
        except Exception as e:
            logger.exception(f"An unexpected error occurred during DeepSeek API call: {e}", exc_info=settings.DEBUG)
            return None
    return None

# --- Specific AI Task Functions ---

def generate_tasks_from_prd(prd_content: str) -> Optional[str]:
    """
    Uses AI to generate a structured list of tasks from PRD content.

    Args:
        prd_content: The content of the Product Requirements Document.

    Returns:
        A JSON string representing the generated tasks, or None on failure.
    """
    logger.info("Generating tasks from PRD using AI...")
    
    # Get the structured prompt from prompts.py
    prompt_message = prompts.get_generate_tasks_prompt(prd_content)

    messages = [{"role": "user", "content": prompt_message}]

    # Use the reasoning model for complex generation
    response = call_deepseek(
        model=settings.DEEPSEEK_REASONING_MODEL,
        messages=messages
    )
    if response:
        logger.info("Successfully generated task structure from PRD.")
        # --- Modification Start: Robust JSON Extraction --- 
        extracted_json = None
        response_str = response.strip()
        try:
            # Determine expected start/end characters (object or list)
            if response_str.find('[') != -1 and response_str.find('[') < response_str.find('{'):
                # Likely a list first
                start_char = '['
                end_char = ']'
            else:
                # Likely an object first or only an object
                start_char = '{'
                end_char = '}'

            # Find the start of the JSON structure
            start_index = response_str.find(start_char)
            # Find the end of the JSON structure (last closing character)
            end_index = response_str.rfind(end_char)
            
            if start_index != -1 and end_index != -1 and end_index > start_index:
                extracted_json = response_str[start_index : end_index + 1]
                # Basic check if it looks like the expected structure
                if not extracted_json.startswith(start_char):
                     extracted_json = None # Reset if extraction doesn't start correctly
            else:
                 logger.debug(f"Could not find valid JSON structure ('{start_char}...{end_char}') in raw response: {response_str[:200]}...")

        except Exception as e:
            logger.warning(f"Error during JSON extraction attempt in generate_tasks_from_prd: {e}")
            extracted_json = None

        # Return the extracted JSON string if successful, otherwise None
        if extracted_json:
            logger.debug("Successfully extracted JSON structure in generate_tasks_from_prd.")
            return extracted_json
        else:
            # Log the final error before returning None
            logger.error(f"AI response did not contain a recognizable JSON structure after extraction attempt: {response_str[:100]}...")
            return None
        # --- Modification End ---
    else:
        logger.error("Failed to generate tasks from PRD using AI (No response received).")
        return None


def expand_task_with_ai(task_title: str, task_description: Optional[str], context: str) -> Optional[str]:
    """
    Uses AI to break down a task into subtasks based on context.

    Args:
        task_title: The title of the parent task.
        task_description: The description of the parent task.
        context: Additional context (e.g., project goals, related tasks).

    Returns:
        A JSON string representing a list of subtasks, or None on failure.
    """
    logger.info(f"Expanding task '{task_title}' into subtasks using AI...")
    # Placeholder prompt:
    prompt_message = f"Please break down the following task into smaller, actionable subtasks based on the provided context. Return the subtasks as a JSON list. Each subtask should have a title and optionally a description and dependencies.\n\nTask Title: {task_title}\nTask Description: {task_description or 'N/A'}\n\nContext:\n{context}"
    messages = [{"role": "user", "content": prompt_message}]

    # Use the reasoning model
    response = call_deepseek(
        model=settings.DEEPSEEK_REASONING_MODEL,
        messages=messages
    )

    if response:
        logger.info(f"Successfully generated subtasks for task '{task_title}'.")
        
        # --- Modification Start: Robust JSON Extraction directly in ai_client ---
        extracted_json = None
        try:
            # Find the start of the JSON list
            start_index = response.find('[')
            # Find the end of the JSON list (last closing bracket)
            end_index = response.rfind(']')
            
            if start_index != -1 and end_index != -1 and end_index > start_index:
                extracted_json = response[start_index : end_index + 1].strip()
                # Basic check if it looks like a list
                if not extracted_json.startswith('['):
                     extracted_json = None # Reset if extraction doesn't start correctly
            else:
                 # Log if structure not found, but don't error yet
                 logger.debug(f"Could not find valid JSON list structure ('[...]') in raw response: {response[:200]}...")

        except Exception as e:
            # Log extraction error, but don't error yet
            logger.warning(f"Error during JSON extraction attempt in ai_client: {e}")
            extracted_json = None

        # Return the extracted JSON string if successful, otherwise None
        if extracted_json:
            logger.debug(f"Successfully extracted JSON list structure in ai_client.")
            return extracted_json
        else:
            # Log the final error before returning None
            logger.error(f"AI response did not contain a recognizable JSON list structure after extraction attempt: {response[:100]}...")
            return None
        # --- Modification End ---
    else:
        logger.error(f"Failed to generate subtasks for task '{task_title}' using AI (No response received).")
        return None

# Example usage (for testing purposes, remove later)
# if __name__ == '__main__':
#     logging.basicConfig(level=logging.DEBUG)
#     # Ensure you have a .env file with DEEPSEEK_API_KEY
#     # test_prd = "Create a login page with username and password fields and a submit button."
#     # generated_tasks_json = generate_tasks_from_prd(test_prd)
#     # print("Generated Tasks JSON:\n", generated_tasks_json)

#     # test_task_title = "Implement user authentication"
#     # test_task_desc = "Handle user login and session management."
#     # test_context = "The application needs secure login. Use JWT tokens."
#     # generated_subtasks_json = expand_task_with_ai(test_task_title, test_task_desc, test_context)
#     # print("\nGenerated Subtasks JSON:\n", generated_subtasks_json)



================================================
FILE: src/ai/prompts.py
================================================
# Defines standardized prompts for interacting with the AI model.

# --- Task Generation from PRD ---

# Instructions for the AI on how to structure the output JSON
# This should align perfectly with the TasksData model in data_models.py
JSON_STRUCTURE_GUIDE = """
The output MUST be a valid JSON object with the following structure:
{
  "meta": {
    "projectName": "string (Extracted or inferred from PRD)",
    "version": "string (e.g., '1.0')"
  },
  "tasks": [
    {
      "id": integer (Start from 1, sequential),
      "title": "string (Concise task title)",
      "description": "string | null (Detailed description)",
      "status": "string (Default: 'pending')",
      "priority": "string ('high', 'medium', or 'low', inferred)",
      "dependencies": [integer | string] (List of IDs of tasks/subtasks this depends on, e.g., [1, "2.1"]),
      "details": "string | null (Any extra notes or context)",
      "test_strategy": "string | null (How to test this task)",
      "subtasks": [
        {
          "id": integer (Start from 1 for each parent, sequential),
          "title": "string (Concise subtask title)",
          "description": "string | null",
          "status": "string (Default: 'pending')",
          # Subtasks inherit priority from parent, no explicit field needed here
          "dependencies": [integer | string] (List of IDs, can depend on parent task's other subtasks e.g., [1] meaning subtask 3.1),
          "details": "string | null",
          "parent_task_id": integer (ID of the parent task) # MUST be included
        }
      ]
    }
  ]
}

- Ensure all fields match the specified types.
- Generate sequential IDs for tasks starting from 1.
- Generate sequential IDs for subtasks starting from 1 *within each parent task*.
- Infer dependencies logically based on the PRD flow. Use integer IDs for main tasks and "parent_id.subtask_id" format for subtask dependencies.
- Infer priority based on importance mentioned or implied in the PRD.
- **Crucially, ensure the 'meta' object contains a 'projectName' field extracted or inferred from the PRD.**
- Keep descriptions concise but informative.
- Provide a basic test strategy if applicable.
- Include the correct `parent_task_id` for all subtasks.
- Output ONLY the JSON object, with no introductory text or explanations.
"""

def get_generate_tasks_prompt(prd_content: str) -> str:
    """Generates the prompt for creating tasks from a PRD."""
    return f"""
Analyze the following Product Requirements Document (PRD) and generate a detailed, structured list of tasks required to implement the features described.

**PRD Content:**
```
{prd_content}
```

**Instructions:**
{JSON_STRUCTURE_GUIDE}
"""

# --- Subtask Expansion ---

# Instructions for the AI on how to structure the subtask list JSON
SUBTASK_JSON_STRUCTURE_GUIDE = """
The output MUST be a valid JSON list containing subtask objects with the following structure:
[
  {
    "title": "string (Concise subtask title)",
    "description": "string | null (Detailed description)",
    # Status, priority, and parent_task_id will be handled by the calling code
    "dependencies": [integer | string] (List of IDs of *other subtasks within the same parent* this depends on, e.g., [1] meaning the first subtask in this list),
    "details": "string | null (Any extra notes or context)"
  }
]

- Break the parent task down into logical, actionable steps.
- Infer dependencies *only between the generated subtasks*. Use the sequential index (starting from 1) of the subtask in the generated list as the dependency ID.
- Keep titles and descriptions focused.
- Output ONLY the JSON list, with no introductory text or explanations.
"""

def get_expand_subtasks_prompt(task_title: str, task_description: str | None, context: str) -> str:
    """Generates the prompt for expanding a task into subtasks."""
    return f"""
Break down the following main task into smaller, actionable subtasks based on the provided context.

**Main Task Title:** {task_title}
**Main Task Description:** {task_description or 'N/A'}

**Context:**
```
{context}
```

**Instructions:**
{SUBTASK_JSON_STRUCTURE_GUIDE}
"""



================================================
FILE: src/cli/main.py
================================================
import typer
from typing import List, Optional
import logging
import os
from rich.panel import Panel
import task_manager.storage_sqlite as storage
import task_manager.core as core
import task_manager.data_models as data_models
import task_manager.parsing as parsing
import task_manager.expansion as expansion
import task_manager.dependencies as dependencies
import task_manager.complexity as complexity
import task_manager.file_generator as file_generator
import cli.ui as ui
from config import settings
from exceptions import ParentTaskNotFoundError # Import exception
import git_utils

# Configure logging
# Set default console level to WARNING to reduce verbosity
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# You can still control the overall application log level via settings if needed,
# for example, by adding a FileHandler with level=settings.LOG_LEVEL.upper()
# Example (add later if file logging is desired):
# if settings.LOG_FILE:
#     file_handler = logging.FileHandler(settings.LOG_FILE)
#     file_handler.setLevel(settings.LOG_LEVEL.upper())
#     file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
#     logging.getLogger().addHandler(file_handler) # Add handler to root logger

app = typer.Typer(help="AI-Powered Task Manager CLI")

def load_task_data() -> data_models.TasksData:
    """Loads task data, handling potential errors."""
    try:
        tasks_data = storage.load_tasks()
        logger.debug(f"Loaded {len(tasks_data.tasks)} tasks.")
        return tasks_data
    except Exception as e:
        ui.console.print(f"[bold red]Error loading tasks: {e}[/]")
        logger.exception("Failed to load tasks.", exc_info=settings.DEBUG)
        raise typer.Exit(code=1)

def save_task_data(tasks_data: data_models.TasksData):
    """Saves task data, handling potential errors."""
    try:
        storage.save_tasks(tasks_data)
        logger.debug(f"Saved {len(tasks_data.tasks)} tasks.")
    except Exception as e:
        ui.console.print(f"[bold red]Error saving tasks: {e}[/]")
        logger.exception("Failed to save tasks.", exc_info=settings.DEBUG)
        raise typer.Exit(code=1)


@app.command(name="list", help="List tasks, optionally filtering by status or priority.")
def list_tasks(
    status: Optional[str] = typer.Option(None, "--status", "-s", help="Filter by task status (e.g., pending, done)."),
    priority: Optional[str] = typer.Option(None, "--priority", "-p", help="Filter by task priority (e.g., high, medium).")
):
    """Lists tasks with optional filtering."""
    logger.info(f"Listing tasks with filters: status='{status}', priority='{priority}'")
    if status and status not in data_models.Status.__args__:
        ui.console.print(Panel(f"[bold red]Invalid status '{status}'.[/bold red]\nValid statuses are: {', '.join(data_models.Status.__args__)}", title="[bold red]Error[/bold red]", border_style="red"))
        raise typer.Exit(code=1)
    if priority and priority not in data_models.Priority.__args__:
        ui.console.print(Panel(f"[bold red]Invalid priority '{priority}'.[/bold red]\nValid priorities are: {', '.join(data_models.Priority.__args__)}", title="[bold red]Error[/bold red]", border_style="red"))
        raise typer.Exit(code=1)
    tasks_data = load_task_data()
    ui.display_tasks_table(tasks_data.tasks, status_filter=status, priority_filter=priority)

@app.command(help="Show details for a specific task or subtask.")
def show(
    task_id: str = typer.Argument(..., help="The ID of the task or subtask (e.g., '1' or '1.2').")
):
    """Shows details for a specific task or subtask."""
    logger.info(f"Showing details for task/subtask ID: {task_id}")
    tasks_data = load_task_data()
    item = core.get_task_by_id(tasks_data.tasks, task_id)
    if item:
        import rich
        rich.print(f"DEBUG in show_command: {item.linked_files}")
        # Also display complexity
        ui.display_task_details(item)
        if isinstance(item, data_models.Task):
            estimated_comp = complexity.estimate_complexity(item)
            ui.console.print(f"[bold]Estimated Complexity:[/bold] {estimated_comp}") # Display complexity after details
        elif isinstance(item, data_models.Subtask):
            estimated_comp = complexity.estimate_complexity(item)
            ui.console.print(f"[bold]Estimated Complexity:[/bold] {estimated_comp}")
        else:
            ui.console.print(f"[bold]Estimated Complexity:[/bold] N/A")
    else:
        ui.console.print(f"ä¸å­˜åœ¨idä¸º {task_id} çš„ task") # User requested message
        logger.warning(f"Task/subtask ID '{task_id}' not found for show command.")
        raise typer.Exit(code=1)

@app.command(name="status", help="Set the status for a task or subtask.")
def set_status_command(
    task_id: str = typer.Argument(..., help="The ID of the task or subtask to update (e.g., '1' or '1.2')."),
    new_status: str = typer.Argument(..., help=f"New status ({', '.join(data_models.Status.__args__)})"),
    propagate: bool = typer.Option(settings.PROPAGATE_STATUS_CHANGE, "--propagate", help="Does cascading affect subtasks (default see configuration file)")
):
    """è®¾ç½®ä»»åŠ¡æˆ–å­ä»»åŠ¡çš„çŠ¶æ€ã€‚"""
    logger.info(f"Attempting to set status to '{new_status}' for ID: {task_id}")
    # Validate status input against the Literal type
    if new_status not in data_models.Status.__args__:
        # Keep error message concise
        ui.console.print(Panel(f"[bold red]Invalid status '{new_status}'.[/bold red]\nValid statuses are: {', '.join(data_models.Status.__args__)}", title="[bold red]Error[/bold red]", border_style="red"))
        raise typer.Exit(code=1)

    tasks_data = load_task_data()
    item = core.get_task_by_id(tasks_data.tasks, task_id)
    if not item:
        ui.console.print(Panel(f"[bold red]Task '{task_id}' not found.[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
        raise typer.Exit(code=1)

    # --- Get old status BEFORE updating ---
    old_status = item.status
    # ---

    # Prevent updating if status is the same
    if old_status == new_status:
        ui.console.print(Panel(f"Status for '{task_id}' is already '{new_status}'. No update needed.", title="Status Update", border_style="yellow"))
        raise typer.Exit()

    if core.set_task_status(tasks_data.tasks, task_id, new_status, propagate=propagate):
        save_task_data(tasks_data)
        # --- Optimized Output with old/new status ---
        ui.console.print(Panel(f"Status for '{task_id}' changed from [yellow]'{old_status}'[/yellow] to [green]'{new_status}'[/green].", title="[bold green]âœ… Status Updated[/bold green]", border_style="green"))
        # ---
    else:
        # Error message should have been logged by core.set_task_status
        ui.console.print(Panel(f"[bold red]Failed to update status for '{task_id}'. Check logs.[/bold red]", title="[bold red]âŒ Update Failed[/bold red]", border_style="red"))
        raise typer.Exit(code=1)


@app.command(name="start", help="Start a task, setting status to 'in-progress' and creating a Git branch.")
def start_task(
    task_id: str = typer.Argument(..., help="The ID of the task to start."),
    propagate: bool = typer.Option(settings.PROPAGATE_STATUS_CHANGE, "--propagate", help="Propagate status change to subtasks.")
):
    """Starts a task, sets its status, and creates a Git branch."""
    logger.info(f"Starting task ID: {task_id}")
    tasks_data = load_task_data()
    item = core.get_task_by_id(tasks_data.tasks, task_id)
    if not item:
        ui.console.print(Panel(f"[bold red]Task '{task_id}' not found.[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
        raise typer.Exit(code=1)

    # Set status to 'in-progress'
    new_status = "in-progress"
    old_status = item.status

    if old_status == new_status:
        ui.console.print(Panel(f"Task '{task_id}' is already in progress.", title="Status Update", border_style="yellow"))
        return  # Do not proceed to create branch
    else:
        if core.set_task_status(tasks_data.tasks, task_id, new_status, propagate=propagate):
            save_task_data(tasks_data)
            ui.console.print(Panel(f"Status for '{task_id}' changed from [yellow]'{old_status}'[/yellow] to [green]'{new_status}'[/green].", title="[bold green]âœ… Status Updated[/bold green]", border_style="green"))
        else:
            ui.console.print(Panel(f"[bold red]Failed to update status for '{task_id}'. Check logs.[/bold red]", title="[bold red]âŒ Update Failed[/bold red]", border_style="red"))
            raise typer.Exit(code=1)

    # Create Git branch
    sanitized_title = "".join(c for c in item.title.lower() if c.isalnum() or c == ' ').replace(' ', '-')
    branch_name = f"feature/task-{item.id}-{sanitized_title}"

    git_utils.create_branch(branch_name)


@app.command(name="complete", help="Complete a task, setting status to 'done' and optionally committing changes.")
def complete_task(
    task_id: str = typer.Argument(..., help="The ID of the task to complete."),
    commit: bool = typer.Option(False, "--commit", "-c", help="Commit changes with a generated message."),
    propagate: bool = typer.Option(settings.PROPAGATE_STATUS_CHANGE, "--propagate", help="Propagate status change to subtasks.")
):
    """Completes a task, sets status to 'done', and can commit changes."""
    logger.info(f"Completing task ID: {task_id}")
    tasks_data = load_task_data()
    item = core.get_task_by_id(tasks_data.tasks, task_id)
    if not item:
        ui.console.print(Panel(f"[bold red]Task '{task_id}' not found.[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
        raise typer.Exit(code=1)

    # Set status to 'done'
    new_status = "done"
    old_status = item.status

    if old_status == new_status:
        ui.console.print(Panel(f"Task '{task_id}' is already done.", title="Status Update", border_style="yellow"))
        return  # Do not proceed further if already done
    else:
        if core.set_task_status(tasks_data.tasks, task_id, new_status, propagate=propagate):
            save_task_data(tasks_data)
            ui.console.print(Panel(f"Status for '{task_id}' changed from [yellow]'{old_status}'[/yellow] to [green]'{new_status}'[/green].", title="[bold green]âœ… Status Updated[/bold green]", border_style="green"))
        else:
            ui.console.print(Panel(f"[bold red]Failed to update status for '{task_id}'. Check logs.[/bold red]", title="[bold red]âŒ Update Failed[/bold red]", border_style="red"))
            raise typer.Exit(code=1)

    # Commit changes if requested
    if commit:
        commit_message = f"feat: Complete task {item.id} - {item.title}"
        if item.description:
            commit_message += f"\n\n{item.description}"

        git_utils.commit_changes(commit_message)

    ui.console.print(Panel(f"Successfully completed task '{task_id}'.", title="[bold green]âœ… Task Completed[/bold green]", border_style="green"))


@app.command(name="next", help="Show the next eligible task to work on.")
def next_task():
    """Finds and displays the next eligible task."""
    logger.info("Finding the next eligible task.")
    tasks_data = load_task_data()
    next_t = core.find_next_task(tasks_data.tasks)
    if next_t:
        ui.console.print("[bold green]Next eligible task:[/bold green]")
        ui.display_task_details(next_t)
    else:
        ui.console.print("âœ… No eligible tasks found to work on right now.")
        logger.info("No eligible tasks found.")

@app.command(name="add", help="Add a new task or subtask.") 
def add_command(
    title: str = typer.Argument(..., help="The title of the task or subtask."),
    # --- Add parent option --- 
    parent_id: Optional[str] = typer.Option(None, "--parent", "-p", help="ID of the parent task to add a subtask to."),
    # --- Add other options corresponding to core functions ---
    description: Optional[str] = typer.Option(None, "--desc", "-d", help="Description for the task/subtask."),
    priority: Optional[str] = typer.Option(None, "--priority", help=f"Priority ({', '.join(data_models.Priority.__args__)}). Default: {settings.DEFAULT_PRIORITY}"),
    dependencies: List[str] = typer.Option([], "--depends", "-dps", help="One or more dependency IDs for this task or subtask. å¯æŒ‡å®šå¤šä¸ªä¾èµ–IDã€‚")
):
    """Adds a new task, or adds a subtask if --parent is specified. æ”¯æŒåˆå§‹åŒ–æ—¶è‡ªå®šä¹‰å¤šä¸ªä¾èµ–ã€‚"""
    tasks_data = load_task_data()
    # Validate priority if provided
    validated_priority = settings.DEFAULT_PRIORITY
    if priority:
        if priority not in data_models.Priority.__args__:
            ui.console.print(Panel(f"[bold red]Invalid priority '{priority}'.[/bold red]\nValid priorities are: {', '.join(data_models.Priority.__args__)}", title="[bold red]Error[/bold red]", border_style="red"))
            raise typer.Exit(code=1)
        validated_priority = priority

    # è§£æžä¾èµ–å‚æ•°ï¼Œæ”¯æŒé€—å·åˆ†éš”å’Œç©ºæ ¼åˆ†éš”
    parsed_dependencies = []
    for dep in dependencies:
        if ',' in dep:
            parsed_dependencies.extend([d.strip() for d in dep.split(',') if d.strip()])
        else:
            parsed_dependencies.append(dep)

    # --- Logic to differentiate task vs subtask --- 
    if parent_id:
        # Adding a subtask
        logger.info(f"Adding new subtask '{title}' to parent {parent_id}")
        try:
            parent_id_int = int(parent_id)
            # Call core.add_subtask
            new_item = core.add_subtask(
                tasks=tasks_data.tasks, 
                parent_task_id=parent_id_int, 
                title=title, 
                description=description,
                priority=validated_priority,
                dependencies=parsed_dependencies
            )
            item_type = "Subtask"
            item_id_str = f"{parent_id_int}.{new_item.id}"
        except ValueError:
             ui.console.print(Panel(f"[bold red]Invalid parent ID format '{parent_id}'. Must be an integer.[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
             raise typer.Exit(code=1)
        except ParentTaskNotFoundError as e:
             ui.console.print(Panel(f"[bold red]{e}[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
             raise typer.Exit(code=1)
        except Exception as e: # Catch other potential errors from core
             ui.console.print(Panel(f"[bold red]Failed to add subtask: {e}[/bold red]", title="[bold red]âŒ Error[/bold red]", border_style="red"))
             logger.exception("Error during add_subtask", exc_info=settings.DEBUG)
             raise typer.Exit(code=1)

    else:
        # Adding a new task
        logger.info(f"Adding new task: {title}")
        try:
            # Call core.add_new_task
            new_item = core.add_new_task(
                tasks=tasks_data.tasks, 
                title=title,
                description=description,
                priority=validated_priority,
                dependencies=parsed_dependencies
            )
            item_type = "Task"
            item_id_str = str(new_item.id)
        except Exception as e: # Catch potential errors from core
             ui.console.print(Panel(f"[bold red]Failed to add task: {e}[/bold red]", title="[bold red]âŒ Error[/bold red]", border_style="red"))
             logger.exception("Error during add_new_task", exc_info=settings.DEBUG)
             raise typer.Exit(code=1)
    # --- End logic differentiation --- 

    if not new_item:
        # This case might be redundant if core functions raise exceptions
        ui.console.print(Panel(f"[bold red]Failed to add {item_type.lower()}. Check logs.[/bold red]", title="[bold red]âŒ Error[/bold red]", border_style="red"))
        raise typer.Exit(code=1)

    # Save the updated data
    save_task_data(tasks_data)
    ui.console.print(Panel(f"Successfully added {item_type} '{title}' with ID [bold cyan]{item_id_str}[/bold cyan]", title=f"[bold green]âœ… {item_type} Added[/bold green]", border_style="green"))


@app.command(name="remove", help="Remove a task or subtask.")
def remove_command(
    task_ids: List[str] = typer.Argument(..., help="The ID(s) of the task(s) or subtask(s) to remove (e.g., '1' or '1.2'). æ”¯æŒå¤šä¸ªIDã€‚")
):
    """åˆ é™¤æŒ‡å®šçš„ä¸€ä¸ªæˆ–å¤šä¸ªä»»åŠ¡æˆ–å­ä»»åŠ¡ã€‚"""
    logger.info(f"Attempting to remove task/subtask IDs: {task_ids}")
    tasks_data = load_task_data()
    # å…¼å®¹é€—å·åˆ†éš”å’Œç©ºæ ¼åˆ†éš”çš„ä»»åŠ¡ID
    parsed_task_ids = []
    for tid in task_ids:
        if ',' in tid:
            parsed_task_ids.extend([t.strip() for t in tid.split(',') if t.strip()])
        else:
            parsed_task_ids.append(tid)
    all_success = True
    failed_ids = []
    for task_id in parsed_task_ids:
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„åˆ é™¤å‡½æ•°ï¼ŒçŽ°åœ¨è¿”å›žæˆåŠŸçŠ¶æ€å’Œä¾èµ–é¡¹åˆ—è¡¨
            success, dependent_items = core.remove_item(tasks_data.tasks, task_id)
            if success:
                # ä¿å­˜æ›´æ”¹
                save_task_data(tasks_data)
                # æ˜¾ç¤ºåˆ é™¤æˆåŠŸä¿¡æ¯
                success_message = f"Successfully removed task/subtask with ID '{task_id}'"
                # å¦‚æžœæœ‰ä¾èµ–é¡¹ï¼Œæ·»åŠ æç¤ºä¿¡æ¯
                if dependent_items:
                    dependent_info = "\n\n[yellow]The following tasks had dependencies on the removed task and have been automatically updated:[/yellow]"
                    for dep_id, dep_title in dependent_items:
                        dependent_info += f"\nâ€¢ Task {dep_id}: {dep_title}"
                    success_message += dependent_info
                # æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯
                ui.console.print(Panel(
                    success_message,
                    title="[bold green]âœ… Removal Complete[/bold green]",
                    border_style="green"
                ))
                logger.info(f"Successfully removed task/subtask ID: {task_id}")
            else:
                all_success = False
                failed_ids.append(task_id)
                # ä¼˜åŒ–è¾“å‡º
                ui.console.print(Panel(
                    f"[bold red]Failed to find task/subtask with ID '{task_id}' to remove.[/bold red]",
                    title="[bold red]âŒ Removal Failed[/bold red]",
                    border_style="red"
                ))
                logger.warning(f"Task/subtask ID '{task_id}' not found for removal.")
        except ValueError as e:
            all_success = False
            failed_ids.append(task_id)
            ui.console.print(Panel(
                f"[bold red]Invalid ID format: {e}[/bold red]",
                title="[bold red]Error[/bold red]",
                border_style="red"
            ))
        except Exception as e:
            all_success = False
            failed_ids.append(task_id)
            ui.console.print(Panel(
                f"[bold red]An error occurred: {e}[/bold red]",
                title="[bold red]Error[/bold red]",
                border_style="red"
            ))
            logger.exception("Unexpected error in remove command")
    if all_success:
        return
    else:
        ui.console.print(Panel(
            f"[bold yellow]éƒ¨åˆ†ä»»åŠ¡/å­ä»»åŠ¡åˆ é™¤å¤±è´¥: {', '.join(failed_ids)}ï¼Œå…¶ä½™å·²åˆ é™¤ï¼ˆå¦‚æœ‰ï¼‰ã€‚[/bold yellow]",
            title="[bold yellow]âš ï¸ éƒ¨åˆ†åˆ é™¤å¤±è´¥[/bold yellow]",
            border_style="yellow"
        ))


# --- AI Powered Commands ---

@app.command(name="prd", help="Parse a PRD file using AI to generate tasks.")
def parse_prd_command(
    prd_filepath: str = typer.Argument(..., help="Path to the Product Requirements Document file.")
):
    """Parses a PRD file to generate and save tasks."""
    logger.info(f"Initiating PRD parsing for: {prd_filepath}") # Keep info log for file
    try:
        # Validate file path
        if not os.path.exists(prd_filepath):
            ui.console.print(Panel(f"[bold red]PRD file not found at '{prd_filepath}'[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
            raise typer.Exit(code=1)
        # Validate file extension
        if not prd_filepath.endswith((".prd", ".txt")):
            ui.console.print(Panel(f"[bold red]Invalid file extension. Only .prd and .txt files are supported.[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
            raise typer.Exit(code=1)
        
        # Optimized Output Start Message
        ui.console.print(Panel(f"Parsing PRD file '{prd_filepath}'...", title="ðŸ¤– PRD Parsing", border_style="blue"))
        success = parsing.parse_prd_and_save(prd_filepath)

        # Optimized Output Final Message
        if success:
            ui.console.print(Panel(f"Successfully parsed PRD and saved tasks to '{settings.TASKS_JSON_PATH}'.", title="[bold green]âœ… PRD Parsed[/bold green]", border_style="green")) 
        else:
            ui.console.print(Panel("[bold red]Failed to parse PRD or save tasks. Check logs for details.[/bold red]", title="[bold red]âŒ PRD Parsing Failed[/bold red]", border_style="red"))
            raise typer.Exit(code=1)

    except FileNotFoundError: # Should be caught by os.path.exists, but keep for safety
        ui.console.print(Panel(f"[bold red]PRD file not found at '{prd_filepath}'[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
        raise typer.Exit(code=1)
    except Exception as e: # Catch other potential errors during setup
        ui.console.print(Panel(f"[bold red]An unexpected error occurred during PRD processing: {e}[/bold red]", title="[bold red]âŒ Error[/bold red]", border_style="red"))
        logger.exception("Unexpected error in parse_prd_command", exc_info=settings.DEBUG)
        raise typer.Exit(code=1)


@app.command(name="expand", help="Expand a task into subtasks using AI.")
def expand_command(
    task_id: str = typer.Argument(..., help="The ID of the parent task to expand (e.g., '1').")
):
    """Expands a task into subtasks using AI and saves the result."""
    logger.info(f"Initiating AI expansion for task ID: {task_id}") # Keep info log for file

    # Basic validation for task ID format (main task only)
    if '.' in task_id:
        ui.console.print(Panel("[bold red]Cannot expand a subtask. Please provide a main task ID.[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
        raise typer.Exit(code=1)
    try:
        task_id_int = int(task_id) # Check if it's a valid integer
    except ValueError:
         ui.console.print(Panel(f"[bold red]Invalid task ID format: '{task_id}'. Must be an integer.[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
         raise typer.Exit(code=1)

    tasks_data = load_task_data()
    task = core.get_task_by_id(tasks_data.tasks, task_id)
    if not task:
        ui.console.print(Panel(f"[bold red]Task '{task_id}' not found.[/bold red]", title="[bold red]Error[/bold red]", border_style="red"))
        raise typer.Exit(code=1)

    # Optimized Output Start Message
    ui.console.print(Panel(f"Expanding task '{task_id}' ({task.title}) using AI...", title="ðŸ¤– Task Expansion", border_style="blue"))
    success = expansion.expand_and_save(task_id)

    # Optimized Output Final Message
    if success:
        ui.console.print(Panel(f"Successfully expanded task '{task_id}'.", title="[bold green]âœ… Expansion Complete[/bold green]", border_style="green")) 
    else:
        # Error should be logged by expand_and_save
        ui.console.print(Panel(f"[bold red]Failed to expand task '{task_id}'. Check logs for details.[/bold red]", title="[bold red]âŒ Expansion Failed[/bold red]", border_style="red"))
        raise typer.Exit(code=1)

@app.command(name="deps", help="Check for circular dependencies in tasks.")
def check_dependencies():
    """Checks for and reports circular dependencies."""
    logger.info("Checking for circular dependencies.")
    tasks_data = load_task_data()
    cycle = dependencies.find_circular_dependencies(tasks_data.tasks)
    if cycle:
        ui.console.print(f"[bold red]Error: Circular dependency detected![/bold red]")
        ui.console.print(f"Cycle path (approx): {' -> '.join(cycle)}")
        raise typer.Exit(code=1)
    else:
        ui.console.print("[bold green]âœ… No circular dependencies found.[/bold green]")

@app.command(name="gen-file", help="Generate a placeholder file for a task.")
def generate_file_command(
    task_id: str = typer.Argument(..., help="The ID of the task to generate a file for (e.g., '1'). Subtasks not supported."),
    output_dir: Optional[str] = typer.Option(None, "--output-dir", "-o", help=f"Directory to save the file (default: {file_generator.DEFAULT_OUTPUT_DIR}).")
):
    """Generates a placeholder file for a specified task."""
    logger.info(f"Attempting to generate file for task ID: {task_id}")

    if '.' in task_id:
        ui.console.print("[bold red]Error:[/bold red] Cannot generate file for a subtask. Please provide a main task ID.")
        raise typer.Exit(code=1)

    try:
        task_id_int = int(task_id)  # Check if it's a valid integer
    except ValueError:
        ui.console.print(f"[bold red]Error:[/bold red] Invalid task ID format: '{task_id}'. Must be an integer.")
        raise typer.Exit(code=1)

    tasks_data = load_task_data()
    task = core.get_task_by_id(tasks_data.tasks, task_id)

    logger.debug(f"Type of object found for ID '{task_id}': {type(task)}") 

    if not task:
        ui.console.print(f"[bold red]Error:[/bold red] Task with ID '{task_id}' not found.")
        raise typer.Exit(code=1)

    # Removed the check for output_dir existence, assuming generate_file_from_task handles it.
    # if output_dir and not os.path.isdir(output_dir):
    #     ui.console.print(f"[bold red]Error:[/bold red] Invalid output directory: '{output_dir}'. Directory not found.")
    #     raise typer.Exit(code=1)

    ui.console.print(f"ðŸ“ Generating file for task '{task.title}'...")
    generated_path = file_generator.generate_file_from_task(task, output_dir=output_dir)

    if generated_path:
        if core.link_file_to_task(tasks_data.tasks, task_id, os.path.abspath(generated_path)):
            save_task_data(tasks_data)
            ui.console.print(f"[bold green]âœ… Successfully generated and linked file: {os.path.abspath(generated_path)}[/bold green]")
        else:
            # This case might happen if the file is already linked, which is unlikely for a newly generated file.
            ui.console.print(f"[bold green]âœ… Successfully generated file: {os.path.abspath(generated_path)}[/bold green]")
            ui.console.print(f"[bold yellow]âš ï¸ Could not auto-link the file to the task. It might already be linked.[/bold yellow]")
        raise typer.Exit(code=0)
    else:
        ui.console.print(f"[bold red]âŒ Failed to generate file for task '{task_id}'. Check logs.[/bold red]")
        raise typer.Exit(code=1)


@app.command(name="link", help="Link a file to a task.")
def link_file_to_task_command(
    task_id: str = typer.Argument(..., help="The ID of the task to link the file to (e.g., '1' or '1.2')."),
    file_path: str = typer.Argument(..., help="The path to the file to link.")
):
    """Links a file to a specified task."""
    logger.info(f"Attempting to link file '{file_path}' to task ID: {task_id}")

    if not os.path.exists(file_path):
        ui.console.print(f"[bold red]Error:[/bold red] File not found at '{file_path}'")
        raise typer.Exit(code=1)

    tasks_data = load_task_data()

    result = core.link_file_to_task(tasks_data.tasks, task_id, os.path.abspath(file_path))

    if result == core.LinkResult.SUCCESS:
        save_task_data(tasks_data)
        ui.console.print(f"[bold green]âœ… Successfully linked file '{os.path.basename(file_path)}' to task '{task_id}'.[/bold green]")
    elif result == core.LinkResult.ALREADY_EXISTS:
        ui.console.print(f"[bold yellow]âš ï¸ File '{os.path.basename(file_path)}' is already linked to task '{task_id}'.[/bold yellow]")
    elif result == core.LinkResult.NOT_FOUND:
        ui.console.print(f"[bold red]âŒ Failed to link file. Task '{task_id}' not found.[/bold red]")
        raise typer.Exit(code=1)
    else:
        ui.console.print(f"[bold red]âŒ Failed to link file due to an unknown error.[/bold red]")
        raise typer.Exit(code=1)


@app.command(name="unlink", help="Unlink a file from a task.")
def unlink_file_from_task_command(
    task_id: str = typer.Argument(..., help="The ID of the task to unlink the file from (e.g., '1' or '1.2')."),
    file_path: str = typer.Argument(..., help="The path to the file to unlink.")
):
    """Unlinks a file from a specified task."""
    logger.info(f"Attempting to unlink file '{file_path}' from task ID: {task_id}")

    tasks_data = load_task_data()

    # The core function will need to handle absolute paths for consistency
    if core.unlink_file_from_task(tasks_data.tasks, task_id, os.path.abspath(file_path)):
        save_task_data(tasks_data)
        ui.console.print(f"[bold green]âœ… Successfully unlinked file '{os.path.basename(file_path)}' from task '{task_id}'.[/bold green]")
    else:
        # Core function logs the specific error
        ui.console.print(f"[bold red]âŒ Failed to unlink file. See logs for details.[/bold red]")
        raise typer.Exit(code=1)


@app.command(name="find-task", help="Find tasks linked to a file.")
def find_task_by_file_command(
    file_path: str = typer.Argument(..., help="The path to the file to find linked tasks for.")
):
    """Finds and displays tasks linked to a specific file."""
    logger.info(f"Attempting to find tasks linked to file: {file_path}")

    if not os.path.exists(file_path):
        ui.console.print(f"[bold red]Error:[/bold red] File not found at '{file_path}'")
        raise typer.Exit(code=1)

    tasks_data = load_task_data()
    linked_items = core.find_tasks_by_file(tasks_data.tasks, file_path)
    print(f"DEBUG: Found {len(linked_items)} linked items.")
    for item in linked_items:
        print(f"DEBUG: Item: {item.title}")

    if not linked_items:
        ui.console.print(f"No tasks found linked to '{os.path.basename(file_path)}'.")
        raise typer.Exit(code=0)

    ui.console.print(f"[bold]Tasks linked to '{os.path.basename(file_path)}':[/bold]")
    for item in linked_items:
        if isinstance(item, data_models.Task):
            ui.console.print(f"  - Task {item.id}: {item.title}")
        elif isinstance(item, data_models.Subtask):
            ui.console.print(f"  - Subtask {item.parent_task_id}.{item.id}: {item.title}")



@app.command(name="report", help="Generate a report (Markdown table or Mermaid diagram).")
def generate_report(
    report_type: str = typer.Argument("markdown", help="Type of report: 'markdown' or 'mermaid'."),
    output_file: Optional[str] = typer.Option(None, "--output", "-o", help="Save report to a file.")
):
    """Generates and displays or saves a report."""
    logger.info(f"Generating '{report_type}' report.")
    tasks_data = load_task_data()

    report_content = ""
    # Optimized Output Start Message
    ui.console.print(f"ðŸ“Š Generating {report_type} report...") 

    if report_type == "markdown":
        report_content = core.generate_markdown_table_tasks_report(tasks_data.tasks)
    elif report_type == "mermaid":
        report_content = dependencies.generate_mermaid_diagram(tasks_data.tasks)
    else:
        ui.console.print(f"[bold red]Error:[/bold red] Invalid report type '{report_type}'. Choose 'markdown' or 'mermaid'.")
        raise typer.Exit(code=1)

    if output_file:
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(report_content)
            # Optimized Output Final Message (File)
            ui.console.print(f"[bold green]âœ… Report saved to '{output_file}'[/bold green]") 
        except IOError as e:
            ui.console.print(f"[bold red]âŒ Error saving report to '{output_file}': {e}[/bold red]")
            raise typer.Exit(code=1)
    else:
        # Print report content using rich console
        ui.console.print("--- Report Start ---")
        ui.console.print(report_content)
        ui.console.print("--- Report End ---")
        # Optimized Output Final Message (Console)
        ui.console.print(f"[bold green]âœ… Report generated successfully.[/bold green]") 

@app.command(name="add-dep", help="Add a dependency to a task.")
def add_dependency_command(
    task_id: str = typer.Argument(..., help="The ID of the task to add dependency to (e.g., '1' or '1.2')."),
    dependency_ids: list[str] = typer.Argument(..., help="One or more dependency IDs to add.")
):
    """ä¸ºæŒ‡å®šä»»åŠ¡ä¸€æ¬¡æ€§æ·»åŠ ä¸€ä¸ªæˆ–å¤šä¸ªä¾èµ–é¡¹ã€‚"""
    logger.info(f"Adding dependencies {dependency_ids} to task {task_id}")
    tasks_data = load_task_data()
    all_success = True
    failed_deps = []
    for dep_id in dependency_ids:
        success = core.add_dependency(tasks_data.tasks, task_id, dep_id)
        if not success:
            all_success = False
            failed_deps.append(dep_id)
    if all_success:
        save_task_data(tasks_data)
        ui.console.print(Panel(
            f"Successfully added dependencies to task '{task_id}': {', '.join(dependency_ids)}",
            title="[bold green]âœ… Dependency(ies) Added[/bold green]",
            border_style="green"
        ))
    else:
        if dependency_ids and len(dependency_ids) > len(failed_deps):
            save_task_data(tasks_data)
        ui.console.print(Panel(
            f"[bold yellow]éƒ¨åˆ†ä¾èµ–æ·»åŠ å¤±è´¥: {', '.join(failed_deps)}ï¼Œå…¶ä½™å·²æ·»åŠ ã€‚[/bold yellow]",
            title="[bold yellow]âš ï¸ éƒ¨åˆ†ä¾èµ–æ·»åŠ å¤±è´¥[/bold yellow]",
            border_style="yellow"
        ))

@app.command(name="remove-dep", help="Remove a dependency from a task.")
def remove_dependency_command(
    task_id: str = typer.Argument(..., help="The ID of the task to remove dependency from (e.g., '1' or '1.2')."),
    dependency_id: str = typer.Argument(..., help="The ID of the dependency to remove (e.g., '1' or '1.2').")
):
    """ä»ŽæŒ‡å®šä»»åŠ¡ä¸­ç§»é™¤ä¾èµ–é¡¹ã€‚"""
    logger.info(f"Removing dependency {dependency_id} from task {task_id}")
    tasks_data = load_task_data()
    
    try:
        success = core.remove_single_dependency(tasks_data.tasks, task_id, dependency_id)
        if success:
            save_task_data(tasks_data)
            ui.console.print(Panel(
                f"Successfully removed dependency '{dependency_id}' from task '{task_id}'",
                title="[bold green]âœ… Dependency Removed[/bold green]",
                border_style="green"
            ))
        else:
            ui.console.print(Panel(
                f"[bold red]Failed to remove dependency '{dependency_id}' from task '{task_id}'.[/bold red]",
                title="[bold red]âŒ Remove Dependency Failed[/bold red]",
                border_style="red"
            ))
            return
            
    except Exception as e:
        ui.console.print(Panel(
            f"[bold red]An error occurred: {e}[/bold red]",
            title="[bold red]Error[/bold red]",
            border_style="red"
        ))
        logger.exception("Unexpected error in remove-dep command")
        return

if __name__ == "__main__":
    # Ensure tasks directory exists (optional, storage might handle this)
    tasks_dir = os.path.dirname(settings.TASKS_JSON_PATH)
    if tasks_dir and not os.path.exists(tasks_dir):
        try:
            os.makedirs(tasks_dir)
            logger.info(f"Created tasks directory: {tasks_dir}")
        except OSError as e:
            logger.error(f"Could not create tasks directory {tasks_dir}: {e}")
            # Decide if this is fatal or not
    app()



================================================
FILE: src/cli/ui.py
================================================
from typing import List, Optional, Union
from rich.console import Console, Group
from rich.table import Table
from rich.panel import Panel
from rich.text import Text
# from rich.tree import Tree # No longer using Tree for details

from task_manager.data_models import Task, Subtask

console = Console()

def get_status_color(status: str) -> str:
    """Return a color based on status."""
    return {
        "pending": "yellow",
        "in-progress": "blue",
        "done": "green",
        "blocked": "red",
        "deferred": "grey70",
        "review": "magenta"
    }.get(status, "white")

def get_priority_color(priority: str) -> str:
    """Return a color based on priority."""
    return {
        "high": "red",
        "medium": "yellow",
        "low": "green"
    }.get(priority, "white")

def get_dep_str(deps: List[Union[int, str]]) -> str:
    """Format dependencies list for display."""
    return ", ".join(str(d) for d in deps) if deps else "None"

def display_tasks_table(tasks: List[Task], status_filter: Optional[str] = None, priority_filter: Optional[str] = None):
    """Display tasks in a rich table format with optional filters."""
    if not tasks:
        console.print("\n[bold]No tasks found.[/bold]\n")
        return

    table = Table(show_header=True, header_style="bold", title="Tasks") # Add a main title for clarity
    table.add_column("ID", style="dim")
    table.add_column(header="Title") # Explicitly use header= parameter
    table.add_column("Status")
    table.add_column("Priority")
    table.add_column("Dependencies")
    table.add_column("Subtasks", justify="right")

    for task in tasks:
        # Filter subtasks first
        filtered_subtasks = []
        if task.subtasks:
            filtered_subtasks = [
                st for st in task.subtasks
                if (not status_filter or st.status == status_filter)
            ]

        # Determine if the main task should be shown
        task_matches = (not status_filter or task.status == status_filter) and \
                       (not priority_filter or task.priority == priority_filter)

        # Show the task if it matches or if it has subtasks that match
        if task_matches or filtered_subtasks:
            # Format status with color
            status_str = f"[{get_status_color(task.status)}]{task.status}[/]"
            priority_str = f"[{get_priority_color(task.priority)}]{task.priority}[/]"

            table.add_row(
                str(task.id),
                task.title,
                status_str,
                priority_str,
                get_dep_str(task.dependencies),
                str(len(task.subtasks))
            )

            # Add subtasks with indentation if they exist
            subtasks_to_display = filtered_subtasks if (status_filter or priority_filter) else task.subtasks
            for subtask in subtasks_to_display:
                subtask_status = f"[{get_status_color(subtask.status)}]{subtask.status}[/]"
                table.add_row(
                    f"â””â”€ {task.id}.{subtask.id}",
                    f"  {subtask.title}",
                    subtask_status,
                    "N/A",
                    get_dep_str(subtask.dependencies),
                    ""
                )

    console.print("\n[bold]Tasks:[/bold]")
    console.print(table)
    console.print()

def display_task_details(item: Union[Task, Subtask]):
    """Display detailed information about a task or subtask."""
    is_subtask = isinstance(item, Subtask)
    item_id = f"{item.parent_task_id}.{item.id}" if is_subtask else str(item.id)
    title = f"[bold]Details for {'Subtask' if is_subtask else 'Task'} {item_id}: {item.title}[/]"

    renderables = []

    # --- Basic Info ---
    basic_info_table = Table.grid(padding=(0, 1))
    basic_info_table.add_column()
    basic_info_table.add_column()
    basic_info_table.add_row("[bold]Status:[/]", f"[{get_status_color(item.status)}]{item.status}[/]")
    if not is_subtask:
        basic_info_table.add_row("[bold]Priority:[/]", f"[{get_priority_color(item.priority)}]{item.priority}[/]")
    renderables.append(Panel(basic_info_table, title="ðŸ“‹ Basic Information", title_align="left", border_style="blue"))

    # --- Description ---
    if item.description:
        renderables.append(Panel(Text(item.description, justify="left"), title="ðŸ“ Description", title_align="left", border_style="blue"))

    # --- Dependencies ---
    if item.dependencies:
         renderables.append(Panel(get_dep_str(item.dependencies), title="ðŸ”— Dependencies", title_align="left", border_style="blue"))

    # --- Linked Files ---
    if item.linked_files:
        linked_files_str = "\n".join([f"- {file}" for file in item.linked_files])
        renderables.append(Panel(linked_files_str, title="ðŸ“Ž Linked Files", title_align="left", border_style="blue"))


    # --- Additional Details ---
    if item.details:
        renderables.append(Panel(Text(item.details, justify="left"), title="â„¹ï¸ Additional Details", title_align="left", border_style="blue"))

    # --- Test Strategy (Tasks only) ---
    if not is_subtask and item.test_strategy:
        renderables.append(Panel(Text(item.test_strategy, justify="left"), title="ðŸ§ª Test Strategy", title_align="left", border_style="blue"))

    # --- Subtasks (Tasks only) ---
    if not is_subtask and hasattr(item, 'subtasks') and item.subtasks:
        subtask_table = Table(show_header=False, box=None, padding=0)
        subtask_table.add_column("ID", style="dim")
        subtask_table.add_column("Title")
        subtask_table.add_column("Status")
        for st in item.subtasks:
            status_str = f"[{get_status_color(st.status)}]{st.status}[/]"
            subtask_table.add_row(f"{item.id}.{st.id}", st.title, status_str)
        renderables.append(Panel(subtask_table, title=f"ðŸ“‘ Subtasks ({len(item.subtasks)})", title_align="left", border_style="blue"))

    # Group all renderables under the main title panel
    console.print(Panel(Group(*renderables), title=title, border_style="green", expand=False))
    console.print()



================================================
FILE: src/config/__init__.py
================================================
 


================================================
FILE: src/config/settings.py
================================================
import os
import logging
from dotenv import load_dotenv

# æ—¥å¿—ç­‰çº§ç±»åž‹
# LogLevel = Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
# ä¼˜å…ˆçº§ç±»åž‹
# Priority = Literal["low", "medium", "high"]

# åŠ è½½ .env æ–‡ä»¶ï¼ˆè·¯å¾„ç›¸å¯¹äºŽæœ¬æ–‡ä»¶ï¼‰
dotenv_path = os.path.join(os.path.dirname(__file__), '../../.env')
load_dotenv(dotenv_path=dotenv_path)

# --- DeepSeek é…ç½® ---
# API å¯†é’¥ï¼ˆå¿…å¡«ï¼‰
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY', None)
# API åŸºç¡€åœ°å€
DEEPSEEK_BASE_URL = os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')
# é€šç”¨æ¨¡åž‹
DEEPSEEK_GENERAL_MODEL = os.getenv('DEEPSEEK_GENERAL_MODEL', 'deepseek-chat')
# æŽ¨ç†/ç¼–ç æ¨¡åž‹
DEEPSEEK_REASONING_MODEL = os.getenv('DEEPSEEK_REASONING_MODEL', 'deepseek-reasoner')

# --- æ•°æ®åº“è®¾ç½® ---
DATABASE_URL = os.getenv('DATABASE_URL', 'sqlite:///:memory:')

# --- AI é€šç”¨è®¾ç½® ---
# AI å“åº”æœ€å¤§ token æ•°
AI_MAX_TOKENS = int(os.getenv('AI_MAX_TOKENS', 8192))
# AI æ¸©åº¦å‚æ•°
AI_TEMPERATURE = float(os.getenv('AI_TEMPERATURE', 0.7))

# --- åº”ç”¨è®¾ç½® ---
# ä¸»ä»»åŠ¡æ–‡ä»¶è·¯å¾„
TASKS_JSON_PATH = os.path.abspath(os.getenv('TASKS_JSON_PATH', 'tasks.json'))
# ä»»åŠ¡æ–‡ä»¶å¤¹è·¯å¾„
TASKS_DIR_PATH = os.getenv('TASKS_DIR_PATH', 'tasks')
# æ—¥å¿—ç­‰çº§
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')  # DEBUG, INFO, WARNING, ERROR, CRITICAL
# é»˜è®¤ä¼˜å…ˆçº§
DEFAULT_PRIORITY = os.getenv('DEFAULT_PRIORITY', 'medium')  # low, medium, high
# é»˜è®¤å­ä»»åŠ¡æ•°
DEFAULT_SUBTASKS = int(os.getenv('DEFAULT_SUBTASKS', 3))
# é¡¹ç›®åç§°
PROJECT_NAME = os.getenv('PROJECT_NAME', 'My Python Task Manager')
# é¡¹ç›®ç‰ˆæœ¬
PROJECT_VERSION = os.getenv('PROJECT_VERSION', '0.1.0')
# æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
DEBUG = os.getenv('DEBUG', 'False').lower() in ('true', '1', 'yes')
# æ˜¯å¦ä¸»ä»»åŠ¡çŠ¶æ€å˜æ›´æ—¶çº§è”å½±å“å­ä»»åŠ¡
# é»˜è®¤å€¼ä¸ºFalseï¼Œå³ä¸çº§è”ï¼ˆå¦‚éœ€çº§è”è¯·åœ¨.envä¸­è®¾ç½®PROPAGATE_STATUS_CHANGE=Trueï¼‰
PROPAGATE_STATUS_CHANGE = os.getenv('PROPAGATE_STATUS_CHANGE', 'False').lower() in ('true', '1', 'yes')

# --- ä¸´æ—¶è°ƒè¯•æ—¥å¿— ---
config_logger = logging.getLogger("config.settings")
if DEEPSEEK_API_KEY:
    config_logger.debug(f"[CONFIG DEBUG] Loaded DEEPSEEK_API_KEY: {DEEPSEEK_API_KEY[:5]}...{DEEPSEEK_API_KEY[-4:]}")
else:
    config_logger.debug("[CONFIG DEBUG] DEEPSEEK_API_KEY not found.")

# ç”¨æ³•ç¤ºä¾‹ï¼šfrom src.config import settings
# print(DEEPSEEK_API_KEY)

# å°è£…ä¸ºSettingsç±»ï¼Œä¾¿äºŽå¤–éƒ¨ç»Ÿä¸€å¯¼å…¥
class Settings:
    def __init__(self):
        self.DEEPSEEK_API_KEY = DEEPSEEK_API_KEY
        self.DEEPSEEK_BASE_URL = DEEPSEEK_BASE_URL
        self.DEEPSEEK_GENERAL_MODEL = DEEPSEEK_GENERAL_MODEL
        self.DEEPSEEK_REASONING_MODEL = DEEPSEEK_REASONING_MODEL
        self.DATABASE_URL = DATABASE_URL
        self.AI_MAX_TOKENS = AI_MAX_TOKENS
        self.AI_TEMPERATURE = AI_TEMPERATURE
        self.TASKS_JSON_PATH = TASKS_JSON_PATH
        self.TASKS_DIR_PATH = TASKS_DIR_PATH
        self.LOG_LEVEL = LOG_LEVEL
        self.DEFAULT_PRIORITY = DEFAULT_PRIORITY
        self.DEFAULT_SUBTASKS = DEFAULT_SUBTASKS
        self.PROJECT_NAME = PROJECT_NAME
        self.PROJECT_VERSION = PROJECT_VERSION
        self.DEBUG = DEBUG
        self.PROPAGATE_STATUS_CHANGE = PROPAGATE_STATUS_CHANGE

# å¯¼å‡ºsettingså®žä¾‹ï¼Œä¾›å¤–éƒ¨import
settings = Settings()



================================================
FILE: src/task_manager/__init__.py
================================================
# This file makes Python treat the directory 'task_manager' as a package. 


================================================
FILE: src/task_manager/complexity.py
================================================
import logging
from typing import Union, Optional

# Use absolute imports
from task_manager.data_models import Task, Subtask

logger = logging.getLogger(__name__)

# Define complexity levels (example)
COMPLEXITY_LOW = "Low"
COMPLEXITY_MEDIUM = "Medium"
COMPLEXITY_HIGH = "High"
COMPLEXITY_UNKNOWN = "Unknown"

def estimate_complexity(item: Union[Task, Subtask]) -> str:
    """
    Estimates the complexity of a task or subtask based on simple heuristics.

    Args:
        item: The Task or Subtask object.

    Returns:
        A string representing the estimated complexity level.
    """
    logger.debug(f"Estimating complexity for item: {item.title}")

    score = 0

    # Factor 1: Description length
    if item.description and len(item.description) > 100:
        score += 1
    if item.description and len(item.description) > 300:
        score += 1

    # Factor 2: Number of dependencies
    if item.dependencies:
        score += len(item.dependencies)

    # Factor 3: Number of subtasks (for Tasks only)
    if isinstance(item, Task) and item.subtasks:
        score += len(item.subtasks)
        # Optionally add complexity based on subtask complexity? (Too complex for now)

    # Factor 4: Explicit details or test strategy
    if item.details:
        score += 1
    if isinstance(item, Task) and item.test_strategy:
        score += 1

    # Determine level based on score (example thresholds)
    if score == 0:
        complexity = COMPLEXITY_LOW
    elif score <= 3:
        complexity = COMPLEXITY_MEDIUM
    else:
        complexity = COMPLEXITY_HIGH

    logger.debug(f"Estimated complexity for '{item.title}' (Score: {score}): {complexity}")
    return complexity

# Example usage (for testing, remove later)
# if __name__ == '__main__':
#     from data_models import Task, Subtask
#     logging.basicConfig(level=logging.DEBUG)

#     task_simple = Task(id=1, title="Simple Task")
#     print(f"{task_simple.title}: {estimate_complexity(task_simple)}") # Expected: Low

#     task_medium = Task(id=2, title="Medium Task", description="This is a description over 100 chars long.", dependencies=[1], subtasks=[Subtask(id=1, title="Sub", parent_task_id=2)])
#     print(f"{task_medium.title}: {estimate_complexity(task_medium)}") # Expected: Medium (1 + 1 + 1 = 3)

#     task_complex = Task(id=3, title="Complex Task", description="Very long description..." * 10, dependencies=[1, 2], subtasks=[Subtask(id=1, title="S1", parent_task_id=3), Subtask(id=2, title="S2", parent_task_id=3)], details="Some details", test_strategy="Manual test")
#     print(f"{task_complex.title}: {estimate_complexity(task_complex)}") # Expected: High (2 + 2 + 2 + 1 + 1 = 8)

